{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dacon주관 금융문자분석 경진대회\n",
    "[대회 개요](https://dacon.io/competitions/official/235401/overview/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_  = pd.read_csv(r\"C:\\Users\\wkd72\\Desktop\\19-2\\filedown\\public_test.csv\", encoding='UTF-8')\n",
    "train = pd.read_csv(r\"C:\\Users\\wkd72\\Desktop\\19-2\\filedown\\train.csv\", encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year_month</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340000</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>XXX고객님! 안녕하세요? 새롭게 시작하는 한 주 행복 가득하시길 기원합니다. 지난...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340001</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>긴급 안내  XXX은행 가락동 지점  - 헬리오XXX 기본XXX    대출이자를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>340002</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>XXX 고객님 안녕하세요올해는 미세먼지가 유난인거 같습니다.엊그제 새해가 시작된거같...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340003</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>XXX 고객님찾아온 행운을 잡으셨나요? 못잡으셨다면 이번에 다시 잡으시길 기원합니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340004</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>XXX 고객님새해 복 많이 받으세요 XXX은행 코스트코 퇴직연금 담당자입니다.  고...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id year_month                                               text\n",
       "0  340000    2019-01  XXX고객님! 안녕하세요? 새롭게 시작하는 한 주 행복 가득하시길 기원합니다. 지난...\n",
       "1  340001    2019-01   긴급 안내  XXX은행 가락동 지점  - 헬리오XXX 기본XXX    대출이자를 ...\n",
       "2  340002    2019-01  XXX 고객님 안녕하세요올해는 미세먼지가 유난인거 같습니다.엊그제 새해가 시작된거같...\n",
       "3  340003    2019-01  XXX 고객님찾아온 행운을 잡으셨나요? 못잡으셨다면 이번에 다시 잡으시길 기원합니다...\n",
       "4  340004    2019-01  XXX 고객님새해 복 많이 받으세요 XXX은행 코스트코 퇴직연금 담당자입니다.  고..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\wkd72\\\\Desktop\\\\19-2\\\\공모전\\\\text\\\\filedown\"\n",
    "data = pd.read_csv(\"{}/train.csv\".format(path))\n",
    "data = data[[\"text\", \"smishing\"]]\n",
    "test = pd.read_csv(\"{}/public_test.csv\".format(path))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test set 비율 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2050)\n",
    "N= 20000\n",
    "N_test = 2000\n",
    "index = random.sample(range(1, data.shape[0]),N )\n",
    "index_test = random.sample(range(1, data.shape[0]),N_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[index,:].copy().reset_index(drop=True)\n",
    "train_test = data.iloc[index_test,:].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def preprocessing(text_data, okt, remove_stopwords=False, stop_words=[]):\n",
    "    '''\n",
    "    remove_stopwords: 불용어 제거 여부 선택\n",
    "    stop_words: 제거할 불용어 리스트\n",
    "    '''\n",
    "    text_data_refined = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", text_data)\n",
    "    text_data_okt = okt.pos(text_data_refined, stem=True)\n",
    "    # 불용어 제거 부분 추가\n",
    "    return text_data_okt\n",
    "\n",
    "#%%\n",
    "# =============================================================================\n",
    "# 한글만을 추출\n",
    "# =============================================================================\n",
    "okt = Okt()\n",
    "clean_text = []\n",
    "#for i in range(len(train_data[\"text\"])):\n",
    "for i in range(N_test):\n",
    "    text = train_test[\"text\"][i]\n",
    "    if type(text) == str: # 비어있는 데이터인지 확인\n",
    "        clean_text.append(preprocessing(text, okt))\n",
    "    else:\n",
    "        clean_text.append([]) # 비어있는 데이터\n",
    "    #printProgress(i, len(train_data[\"text\"]) - 1, \"preprocessing: \", \"Complete\", 1, 50)\n",
    "\n",
    "#%%\n",
    "# =============================================================================\n",
    "# 특정 품사만을 선택\n",
    "# =============================================================================\n",
    "useful_tag = ['Noun', 'Verb', 'Adje', 'Adve', 'Suffix']\n",
    "clean_testing = []\n",
    "for i in range(len(clean_text)):\n",
    "    clean_testing.append([x for (x, y) in clean_text[i] if y in useful_tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 학습\n",
    "> Simple LSTM 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\wkd72\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "WARNING:tensorflow:From c:\\users\\wkd72\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "15000/15000 [==============================] - 32s 2ms/sample - loss: 0.0933 - acc: 0.9821 - val_loss: 0.0137 - val_acc: 0.9990\n",
      "Epoch 2/3\n",
      "15000/15000 [==============================] - 30s 2ms/sample - loss: 0.0083 - acc: 0.9993 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "15000/15000 [==============================] - 30s 2ms/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0020 - val_acc: 0.9998\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train)\n",
    "sequences = tokenizer.texts_to_sequences(clean_train)\n",
    "\n",
    "X_data = sequences\n",
    "y_train = train['smishing']\n",
    "\n",
    "max_len = 200\n",
    "# 전체 데이터셋의 길이는 200으로 맞춥니다.\n",
    "X_data = pad_sequences(X_data, maxlen=max_len,truncating = 'post')\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index)+1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32, input_length = max_len)) # 임베딩 벡터의 차원은 32\n",
    "model.add(LSTM(32)) # RNN 셀의 hidden_size는 32\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_data, y_train, epochs=3, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_testing)\n",
    "sequences = tokenizer.texts_to_sequences(clean_testing)\n",
    "\n",
    "X_testing = sequences\n",
    "pad_testing = pad_sequences(X_testing, maxlen=max_len, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_testing\n",
    "y_test = train_test['smishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2070 - acc: 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.963"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9514036016949153\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "result = model.predict(X_test)\n",
    "print(metrics.roc_auc_score(y_test, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "clean_text_test = []\n",
    "#for i in range(len(train_data[\"text\"])):\n",
    "for i in range(test.shape[0]):\n",
    "    text = test[\"text\"][i]\n",
    "    if type(text) == str: # 비어있는 데이터인지 확인\n",
    "        clean_text_test.append(preprocessing(text, okt))\n",
    "    else:\n",
    "        clean_text_test.append([]) # 비어있는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tag = ['Noun', 'Verb', 'Adje', 'Adve', 'Suffix']\n",
    "clean_test = []\n",
    "for i in range(len(clean_text_test)):\n",
    "    clean_test.append([x for (x, y) in clean_text_test[i] if y in useful_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_test)\n",
    "sequences = tokenizer.texts_to_sequences(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5636\n"
     ]
    }
   ],
   "source": [
    "word_to_index = tokenizer.word_index\n",
    "#print(word_to_index)\n",
    "vocab_size = len(word_to_index)+1\n",
    "print(vocab_size)\n",
    "\n",
    "X_data = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "# 전체 데이터셋의 길이는 200으로 맞춥니다.\n",
    "data_test = pad_sequences(X_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "smishing_result = model.predict(data_test)\n",
    "smishing_result = smishing_result.reshape(smishing_result.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00012481, 0.09851322, 0.0001353 , ..., 0.00015953, 0.00011337,\n",
       "       0.00014754], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smishing_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_excel = pd.DataFrame({'id' : test['id'], 'smishing' : smishing_result })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_excel.to_csv(\"result.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
